\section{Diskussion}
In dieser Arbeit konnte erfolgreich aufgezeigt werden, dass anhand von öffentlichen Daten und modernen Machine Learning Methoden eine akkurate Schätzung abgeben werden kann.\\[2ex]
%
Das sammeln von öffentlichen Daten war mit Aufwand verbunden. Für jede Immobilienplattformen musste ein eigener Crawler programmiert werden. Es benötigte mehrere Durchläufe, bis alle Crawler die Inserate vollumfänglich sammeln konnten. Die Datenqualität der verschiedenen Immobilienplattformen waren eher dürftig. Es konnten somit nur sehr wenige Kennwerte verwendet werden. Bei einer professionellen Schätzung würden noch diverse andere Kennwerte zum Einsatz kommen. Mit diesen zusätzlichen Informationen könnte die Schätzung noch genauer werden.\\
Ist die Idee über längere Zeit Daten zu sammeln, lohnt es sich, in einen autonomen Crawler Zeit zu investieren. So besteht keine Abhängig vom XPath oder CSS und ist gegenüber einem Redesign der Portale gewappnet. Eine weitere Möglichkeit wäre, die Daten bei einer vorhandenen Immobilienplattform direkt zu beziehen.\\
Mit dem Einsatz der Scrapy-Pipeline konnte zusätzlich an Stabilität gewonnen werden. Bei einem Fehler in der Pipeline, konnte das Inserat trotzdem gespeichert werden, ohne dass weitere Informationen verloren gingen.\\
%
Die Verknüpfung der ortsbezogenen Daten mit den Inseraten kann bei wenigen aufwendig sein. Insbesondere wenn die Gemeindenamen durch Fusionen nicht mehr existierten. Wir haben OpenStreetMap verwendet, um möglichst kostengünstig die Koordinaten zu erhalten. Der Nachteil an OpenStreetMap ist, dass es mehrheitlich nur Städte abdeckt. Hier lohnt sich auf jeden Fall die Google Maps API zu verwenden, da die Adresssuche viel besser ist.\\[2ex]
%
Das Crawlen über ein Proxy lohnt sich, da somit die Gefahr, als Crawler erkannt zu werden, verringert wird. Durch das wir über einen Server ausserhalb der Schweiz kamen, konnten wir nicht alle Plattformen, wie zum Beispiel Comparis, aufrufen.\\
Es ist eventuell sogar sinnvoll nur von einer Plattform die Daten zu sammeln. So müssen nicht die unterschiedlichen Bezeichnungen harmonisiert werden. Zudem sind die Charakteristiken immer einheitlich und können ohne transformation verwendet werden.

Das Untersuchen und Vergleichen von verschiedenen Machine Learning Algorithmen ist nicht einfach. Auch wenn immer der gleiche Random State verwendet wurde, wichen die Resultate marginal voneinander ab. So ist es schwer, eine genaue Aussage über einen Performancegewinn beim Feature Engineering zu machen. Es sollte wenn immer möglich mehrere Durchgänge durchgeführt werden.\\
Eine genauere Analyse des Kaufpreises zeigte, dass es nicht so viele verschiedene Preise gibt. In 73'483 Inseraten befinden sich lediglich 1'970 unterschiedliche Preise. Davon sind die meisten auf die nächsten 1'000 CHF gerundet. Mit einer Abweichung von 10\% des Wohnungsmedianpreises, sind über 12\% aller möglichen Preise abgedeckt. Dies entspricht über 17\% der Inserate. Durch das, lässt sich eine MdAPE von 0\% erkären. So ist zum Beispiel beim KNN Algorithmus gut möglich, dass zwei Inserate, mit dem kleinsten euklidischen Abstand, denselben Preis besitzen wie die gesuchte Immobilie. Auch die restlichen guten Resultate lassen sich auf die kleine Diversität der Preise zurückführen.\\
Nach der Outlier Detection war ersichtlich, dass die MAPE unter 5\% fiel. Das liegt an der stark reduzierten Standardabweichung, die kaum höher als eine halbe Million beträgt. Diese Begebenheiten führen zu einem guten Schätzungsmodell.\\
Zusätzlich muss beachtet werden, dass wir vor allem Standardimmobilien in unserem Datensatz haben. Sprich ausgefallene Inserate können nur schwer geschätzt werden, wenn sie nicht schon bei der Outlier Detection herausgenommen wurden. Somit eignen sich unsere Modell nicht für ausgefallene Immobilien.

Aufgefallen war uns auch, dass je mehr Inserate verwendet wurden, desto bessere Ergebnisse wurden erzielt. Verwendeten wir nur die Hälfte der Inserate, konnten die Baummodelle nur noch etwa 72\% Prozent mit einer Abweichung von maximal 10\% richtig schätzen. Somit gehen wir davon aus, dass bei einem grösseren Datensatz mehrere gleiche Preise für ähnliche Objekte vorhanden sind, was wiederum eine tiefere MAPE wie auch MdAPE mit sich bringt.

Unsere eigen aufgebautes Pipeline-System hat uns beim iterativen Prozess sehr unterstützt. So konnte ohne grossen Aufwand diverse Szenarien durchgerechnet werden, ohne dass dazwischen Code verändert werden musste.
%	
\subsection{Ausblick} 
Es gibt noch viele weitere Machine Learning Algorithmen, die man untersuchen könnte. Unter Anderem wäre der LightGBM wie auch ein Neuronaler Ansatz interessant.\\ 
Für Immobilienmakler müssten sicherlich noch mehr Kennwerte in das Modell miteinbezogen werden um einen vertretbaren Preis schätzen zu können. Zusätzlich könnten weitere ortsbezogene Daten wie Einkaufsmöglichkeiten, Schule oder öffentlicher Verkehranbindung gesammelt und verwendet werden.\\
Um die Laufzeit zu verkürzen, könnten nicht verwendete Features im vorhinein entfernt werden.
