\section{Diskussion}
In dieser Arbeit konnte erfolgreich aufgezeigt werden, dass anhand von öffentlichen Daten und modernen Machine Learning Methoden eine akkurate Schätzung des Kaufpreises abgegeben werden kann.\\[2ex]
%
Das Sammeln von öffentlichen Daten war mit Aufwand verbunden. Für jede Immobilienplattform musste ein eigener Crawler programmiert werden. Es benötigte mehrere Durchläufe, bis alle Crawler die Inserate vollumfänglich sammeln konnten. Die Datenqualität der verschiedenen Immobilienplattformen war eher dürftig. Es konnten somit nur sehr wenige Kennwerte verwendet werden. Bei einer professionellen Schätzung würden noch diverse andere Kennwerte zum Einsatz kommen. Mit diesen zusätzlichen Informationen könnte die Schätzung noch genauer werden.\\
Sollten Daten über längere Zeit gesammelt werden, lohnt es sich, in einen autonomen Crawler Zeit zu investieren. So besteht keine Abhängigkeit vom XPath oder CSS und ist gegenüber einem Redesign der Portale gewappnet. Eine weitere Möglichkeit wäre, die Daten bei einer vorhandenen Immobilienplattform direkt zu kaufen.\\
Mit dem Einsatz der Scrapy-Pipeline konnte zusätzlich an Stabilität gewonnen werden. Bei einem Fehler in der Pipeline, konnte das Inserat trotzdem gespeichert werden, ohne, dass weitere Informationen verloren gingen.\\
%
Die Verknüpfung der ortsbezogenen Daten mit den Inseraten kann bei wenigen aufwendig sein. Insbesondere dann, wenn die Gemeindenamen durch Fusionen nicht mehr existieren. Wir haben OpenStreetMap verwendet, um möglichst kostengünstig die Koordinaten zu erhalten. Der Nachteil an OpenStreetMap ist, dass es mehrheitlich nur Städte abdeckt. Hier lohnt sich auf jeden Fall die Google Maps API zu verwenden, da die Adresssuche viel besser ist.\\[2ex]
%
Das Crawlen über einen Proxy lohnt sich, da somit die Gefahr, als Crawler erkannt zu werden, verringert wird. Dadurch, dass wir über einen Server ausserhalb der Schweiz kamen, konnten wir nicht alle Plattformen, wie zum Beispiel Comparis, aufrufen.\\
Es ist eventuell sogar sinnvoll nur von einer Plattform die Daten zu sammeln. So müssen die unterschiedlichen Bezeichnungen nicht harmonisiert werden. Zudem sind die Charakteristiken immer einheitlich und können ohne Transformation verwendet werden.

Das Untersuchen und Vergleichen von verschiedenen Machine Learning Algorithmen ist nicht einfach. Auch wenn immer der gleiche Random State verwendet wurde, wichen die Resultate marginal voneinander ab. So ist es schwer, eine genaue Aussage über einen Performancegewinn beim Feature Engineering zu machen. Es sollte wenn immer möglich mehrere Durchgänge durchgeführt werden.\\
Eine genauere Analyse des Kaufpreises zeigte, dass es nicht so viele verschiedene Preise gibt. In 73'483 Inseraten befinden sich lediglich 1'970 unterschiedliche Preise. Davon sind die meisten auf die nächsten 1'000 CHF gerundet. Mit einer Abweichung von 10\% des Wohnungsmedianpreises, sind über 12\% aller möglichen Preise abgedeckt. Dies entspricht über 17\% der Inserate. Dadurch lässt sich eine MdAPE von 0\% erklären. So ist es zum Beispiel beim KNN Algorithmus gut möglich, dass zwei Inserate mit dem kleinsten euklidischen Abstand, denselben Preis besitzen, wie die gesuchte Immobilie. Auch die restlichen guten Resultate lassen sich auf die kleine Diversität der Preise zurückführen.\\
Nach der Outlier Detection war ersichtlich, dass die MAPE unter 5\% fiel. Das liegt an der stark reduzierten Standardabweichung, die kaum höher als eine halbe Million Franken beträgt. Diese Begebenheiten führen zu einem guten Schätzungsmodell.\\
Zusätzlich muss beachtet werden, dass wir vor allem Standardimmobilien in unserem Datensatz haben. Sprich ausgefallene Inserate können nur schwer geschätzt werden, wenn sie nicht schon bei der Outlier Detection herausgenommen wurden. Somit eignet sich unser Modell nicht für ausgefallene Immobilien.

Aufgefallen war uns auch, dass, je mehr Inserate verwendet wurden, desto bessere waren die Ergebnisse. Verwendeten wir nur die Hälfte der Inserate, konnten die Baummodelle nur noch etwa 72\% Prozent mit einer Abweichung von maximal 10\% richtig schätzen. Somit gehen wir davon aus, dass bei einem grösseren Datensatz mehrere gleiche Preise für ähnliche Objekte vorhanden sind, was wiederum eine tiefere MAPE wie auch MdAPE mit sich bringt.

Unsere selbst aufgebautes Pipeline-System hat uns beim iterativen Prozess sehr unterstützt. So konnten ohne grossen Aufwand diverse Szenarien durchgerechnet werden, ohne, dass dazwischen Code verändert werden musste.
%
\subsection{Ausblick}
Es gibt noch viele weitere Machine Learning Algorithmen, die man untersuchen könnte. Unter anderem wäre der LightGBM, wie auch ein Neuronaler Ansatz interessant.\\
Für Immobilienmakler müssten sicherlich noch mehr Kennwerte in das Modell miteinbezogen werden um einen vertretbaren Preis schätzen zu können. Zusätzlich könnten weitere ortsbezogene Daten wie Einkaufsmöglichkeiten, Schulen oder öffentliche Verkehrsverbindungen gesammelt und verwendet werden.\\
Um die Laufzeit zu verkürzen, könnten nicht verwendete Features im Vorhinein entfernt werden.
